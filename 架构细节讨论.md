```markdown
# λ-Py EAP 控制平面与计算平面交互模式
## 典型场景实现示例

---

## 1. 基础交互模式总览

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         三种核心交互模式                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  模式1: 请求-响应 (Request-Response)                                     │
│  ┌─────────┐    gRPC/HTTP    ┌─────────┐                               │
│  │ Haskell │ ◄──────────────►│ Python  │  适用: 同步推理，低延迟         │
│  │ 控制    │   短连接，有超时 │  计算   │  例: 实时风控，对话响应          │
│  └─────────┘                 └─────────┘                               │
│                                                                         │
│  模式2: 流式传输 (Streaming)                                             │
│  ┌─────────┐    gRPC Stream   ┌─────────┐                              │
│  │ Haskell │ ◄════════════════►│ Python  │  适用: 生成式AI，大输出         │
│  │ 控制    │   双向流，背压控制 │  计算   │  例: LLM文本生成，日志流         │
│  └─────────┘                  └─────────┘                              │
│                                                                         │
│  模式3: 共享内存 (Shared Memory)                                         │
│  ┌─────────┐    mmap/CUDA IPC   ┌─────────┐                            │
│  │ Haskell │ ◄══════════════════►│ Python  │  适用: 超大张量，零拷贝        │
│  │ 控制    │   零序列化，<10μs   │  计算   │  例: 自动驾驶感知，高频交易      │
│  └─────────┘                    └─────────┘                            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 2. 示例一：金融实时风控（请求-响应模式）

### 场景
用户发起转账，Haskell控制平面在10ms内完成风险评估，决定放行或拦截。

### Haskell 控制平面

```haskell
{-# LANGUAGE DataKinds #-}

module RiskControl.Transfer where

import Control.Monad.Freer (Eff, Members)
import Control.Concurrent.Timeout (timeout)
import Data.Money (Money, Currency)
import Data.Risk (RiskScore, RiskLevel(..))

-- 定义效果
data Inference r where
  CallRiskModel :: CustomerProfile -> Transaction -> Inference RiskScore

-- 工作流定义（纯描述，可测试）
evaluateTransfer :: Members '[Inference, Error RiskError, Logging] effs
                 => CustomerId 
                 -> AccountId 
                 -> Money 'USD 2 
                 -> Eff effs TransferDecision
evaluateTransfer customer from amount = do
  -- 1. 本地规则检查（Haskell，<1ms）
  checkLocalRules customer amount
  
  -- 2. 调用Python风险模型（网络，5-8ms）
  profile <- getCustomerProfile customer
  let tx = Transaction from amount timestamp
  riskScore <- callRiskModel profile tx
  
  -- 3. 决策（类型级强制）
  case classifyRisk riskScore of
    Low  -> approveTransfer customer amount
    Medium -> require2FA customer >> approveTransfer customer amount
    High -> declineTransfer "高风险，需人工审核"

-- 解释器实现：gRPC调用Python
runInferenceGRPC :: GRPCConfig -> Eff (Inference ': effs) ~> Eff effs
runInferenceGRPC config = interpret $ \case
  CallRiskModel profile tx -> do
    -- 构建gRPC请求
    let request = RiskRequest
          { customerFeatures = serialize profile
          , transactionFeatures = serialize tx
          , modelVersion = "risk-v3.2.1"  -- 显式版本控制
          }
    
    -- 带熔断的调用
    withCircuitBreaker "risk-model" $ do
      withTimeout (milliseconds 10) $ do  -- 硬超时
        response <- grpcCall (riskEndpoint config) request
        
        -- 验证响应结构（防御性编程）
        case validateResponse response of
          Left err -> do
            logError $ "模型响应无效: " <> show err
            -- 故障降级：保守策略
            return HighRiskScore
          
          Right score -> return score
```

### Python 计算平面

```python
# risk_service.py
import grpc
from concurrent import futures
import torch
import numpy as np
from pydantic import BaseModel, validator

# Pydantic模型：运行时验证输入
class RiskRequest(BaseModel):
    customer_features: dict
    transaction_features: dict
    model_version: str
    
    @validator('transaction_features')
    def validate_amount(cls, v):
        amount = v.get('amount_cents', 0)
        if amount < 0:
            raise ValueError('金额不能为负')
        if amount > 10_000_000_00:  # 1000万
            raise ValueError('金额超限')
        return v

class RiskServicer(risk_pb2_grpc.RiskServiceServicer):
    def __init__(self):
        # 预加载模型到GPU
        self.model = self.load_model('risk-v3.2.1')
        self.model.eval()  # 推理模式
        
    def PredictRisk(self, request: risk_pb2.RiskRequest, context):
        try:
            # 1. 反序列化（验证）
            req = RiskRequest.parse_raw(request.payload)
            
            # 2. 特征工程（向量化）
            features = self.extract_features(
                req.customer_features,
                req.transaction_features
            )
            tensor = torch.tensor(features, dtype=torch.float32).cuda()
            
            # 3. 模型推理（无梯度，节省内存）
            with torch.no_grad():
                logits = self.model(tensor)
                probabilities = torch.softmax(logits, dim=-1)
            
            # 4. 构建响应
            risk_score = probabilities[0].cpu().numpy().tolist()
            
            return risk_pb2.RiskResponse(
                risk_score=risk_score,
                model_version=req.model_version,
                latency_ms=...,
                feature_importance=self.get_shap_values(features)
            )
            
        except Exception as e:
            # 详细错误返回，便于Haskell日志记录
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"推理失败: {str(e)}")
            raise

# 启动：多worker，GPU批处理
def serve():
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=4),
        options=[('grpc.max_send_message_length', 50 * 1024 * 1024)]
    )
    risk_pb2_grpc.add_RiskServiceServicer_to_server(RiskServicer(), server)
    server.add_insecure_port('[::]:50051')
    server.start()
    server.wait_for_termination()
```

### 交互流程图

```
用户请求
   │
   ▼
┌─────────────────┐
│ Haskell         │
│ 1. 解析请求      │◄── 类型安全：Money 'USD 2 保证金额格式正确
│ 2. 本地规则检查  │◄── 黑名单/限额检查，<1ms
│ 3. 构建gRPC请求  │
└────────┬────────┘
         │ gRPC (HTTP/2)
         │ 序列化: Protocol Buffers
         ▼
┌─────────────────┐
│ Python          │
│ 1. Pydantic验证  │◄── 防御性验证，防止恶意输入
│ 2. 特征工程      │
│ 3. GPU推理       │◄── PyTorch，批量优化
│ 4. SHAP可解释性  │
└────────┬────────┘
         │ gRPC Response
         │ 包含: risk_score, feature_importance
         ▼
┌─────────────────┐
│ Haskell         │
│ 1. 验证响应结构  │◄── 防御：模型输出必须符合预期格式
│ 2. 风险分级      │◄── Low/Medium/High 类型级区分
│ 3. 决策执行      │◄── 批准/2FA/拒绝，自动审计
│ 4. 返回结果      │
└─────────────────┘
```

---

## 3. 示例二：LLM对话生成（流式传输模式）

### 场景
用户与AI助手对话，Haskell控制流式输出，Python逐token生成。

### Haskell 控制平面

```haskell
module LLM.Streaming where

import Data.Conduit (ConduitT, yield, await, (.|))
import Network.GRPC.Client (RPCCall, readNext, sendRequest)
import Control.Monad.STM (atomically, TQueue, writeTQueue, readTQueue)

-- 流式工作流
chatStream :: Members '[LLM, Logging, Metrics] effs
           => Conversation 
           -> UserMessage 
           -> ConduitT Text Void (Eff effs) ()  -- 逐字输出
chatStream history userMsg = do
  -- 1. 构建提示（Haskell管理上下文窗口）
  let prompt = buildPrompt history userMsg
  
  -- 2. 安全检查：提示注入检测
  when (detectPromptInjection userMsg) $ do
    yield "检测到不安全输入，请重新表述。"
    return
  
  -- 3. 启动流式调用
  bracket 
    (startStream "gpt-4" prompt)  -- 初始化连接
    closeStream                    -- 清理
    (\stream -> do
      -- 4. 逐token处理
      forever $ do
        maybeToken <- lift $ readNext stream
        
        case maybeToken of
          Nothing -> return ()  -- 流结束
          Just token -> do
            -- 实时内容审核（流中过滤）
            when (not $ isHarmful token) $ do
              yield token  -- 输出给用户
              
              -- 异步：更新对话历史
              lift $ async $ updateHistory history token
              
              -- 异步：记录指标
              lift $ async $ recordMetrics tokenLatency
    )

-- 高级功能：流式工具调用
chatWithTools :: Conversation -> UserMessage -> ConduitT Text Void (Eff effs) ()
chatWithTools history userMsg = do
  -- LLM可能决定调用工具
  let stream = startStreamWithTools "gpt-4" prompt availableTools
  
  processStream stream $ \event -> case event of
    TextToken t -> yield t
    
    ToolCallStart toolName -> do
      yield $ "\n[正在调用工具: " <> toolName <> "]\n"
      
    ToolCallEnd toolName result -> do
      -- 工具结果送回LLM继续生成
      let newPrompt = prompt <> toolResultContext result
      chatWithTools history newPrompt  -- 递归继续
      
    Finish -> return ()
```

### Python 计算平面

```python
# llm_service.py
import grpc
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class LLMServicer(llm_pb2_grpc.LLMServiceServicer):
    def __init__(self):
        self.model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-70b",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-70b"
        )
        
    async def GenerateStream(self, request_iterator, context):
        """双向流：接收提示，返回token流"""
        
        # 收集完整提示（可能分多帧）
        prompt_parts = []
        async for request in request_iterator:
            prompt_parts.append(request.prompt_chunk)
        prompt = "".join(prompt_parts)
        
        # 工具调用检测（函数调用模式）
        tools = json.loads(request.tools_json) if request.tools else []
        
        # 生成
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        streamer = TextIteratorStreamer(self.tokenizer)
        
        # 异步生成（非阻塞）
        generation_kwargs = dict(
            inputs,
            streamer=streamer,
            max_new_tokens=1024,
            temperature=0.7,
            tools=tools,  # 支持工具调用
        )
        
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # 流式返回
        generated_text = ""
        for new_text in streamer:
            generated_text += new_text
            
            # 检测工具调用（特殊token）
            if "<tool>" in generated_text:
                tool_call = parse_tool_call(generated_text)
                yield llm_pb2.StreamResponse(
                    event_type=TOOL_CALL,
                    tool_call=json.dumps(tool_call)
                )
                # 等待Haskell执行工具，结果在下个请求中
            
            else:
                yield llm_pb2.StreamResponse(
                    event_type=TEXT_TOKEN,
                    token=new_text,
                    finish_reason=None
                )
        
        yield llm_pb2.StreamResponse(
            event_type=FINISH,
            finish_reason="stop"
        )

# 启动
async def serve():
    server = grpc.aio.server(
        options=[('grpc.max_concurrent_streams', 100)]
    )
    llm_pb2_grpc.add_LLMServiceServicer_to_server(LLMServicer(), server)
    server.add_insecure_port('[::]:50052')
    await server.start()
    await server.wait_for_termination()
```

### 交互流程图

```
用户输入
   │
   ▼
┌─────────────────┐
│ Haskell         │
│ 1. 上下文管理    │◄── 维护对话历史，截断策略
│ 2. 提示注入检测  │◄── 安全检查，拒绝恶意输入
│ 3. 发送gRPC流    │
└────────┬────────┘
         │ gRPC双向流
         │ 帧1: prompt_chunk
         │ 帧2: tools_available
         ▼
┌─────────────────┐
│ Python          │
│ 1. 接收完整提示  │
│ 2. LLM生成      │◄── 逐token流式输出
│ 3. 工具调用检测  │◄── 特殊token解析
└────────┬────────┘
         │ 流式响应
         │ 帧1: {type: TEXT, token: "Hello"}
         │ 帧2: {type: TEXT, token: "!"}
         │ 帧3: {type: TOOL_CALL, name: "get_weather"}
         ▼
┌─────────────────┐
│ Haskell         │
│ 1. 实时输出token │◄── 用户立即看到"Hello!"
│ 2. 检测到工具调用 │
│ 3. 执行工具      │◄── 调用天气API（Haskell端）
│ 4. 结果送回Python│◄── 新gRPC请求
│ 5. 继续生成      │
└─────────────────┘
```

---

## 4. 示例三：自动驾驶感知（共享内存模式）

### 场景
高频传感器数据（10GB/s），Haskell规划器需要<1ms延迟获取Python感知结果。

### Haskell 控制平面

```haskell
{-# LANGUAGE LinearTypes #-}
{-# LANGUAGE MagicHash #-}

module Autonomy.Perception where

import Foreign.Ptr (Ptr, castPtr)
import Foreign.ForeignPtr (ForeignPtr, newForeignPtr_)
import GHC.Exts (RealWorld)
import Data.Primitive.ByteArray (MutableByteArray#)

-- 共享内存结构（与Python约定布局）
data PerceptionShm = PerceptionShm
 { shmHeader :: Ptr ShmHeader      -- 原子状态/锁
 , shmPointCloud :: Ptr Float      -- 激光雷达点云 (N x 4)
 , shmImage :: Ptr Word8           -- 相机图像 (H x W x 3)
 , shmOutput :: Ptr Detection      -- 检测结果
 }

data ShmHeader = ShmHeader
 { hWriteSeq :: Ptr AtomicInt      -- Python写入序列号
 , hReadSeq :: Ptr AtomicInt       -- Haskell读取序列号
 , hTimestamp :: Ptr Int64         -- 传感器时间戳
 , hDataReady :: Ptr AtomicBool    -- 数据就绪标志
 }

-- 线性类型确保正确使用
acquirePerception :: PerceptionShm 
                  ->. IO (Either PerceptionError PerceptionData)
acquirePerception shm = do
  -- 1. 检查新数据（无锁，seq号比较）
  writeSeq <- atomicLoad (hWriteSeq shm)
  readSeq <- atomicLoad (hReadSeq shm)
  
  when (writeSeq == readSeq) $ do
    return $ Left NoNewData  -- Python尚未处理完
  
  -- 2. 等待数据就绪（自旋，<1μs）
  spinWait (hDataReady shm)
  
  -- 3. 零拷贝读取（mmap，无序列化）
  let detections = unsafeCastArray (shmOutput shm)
  
  -- 4. 原子更新读取序列号
  atomicStore (hReadSeq shm) writeSeq
  
  -- 5. 验证时间戳（防延迟数据）
  ts <- atomicLoad (hTimestamp shm)
  now <- getMonotonicTime
  when (now - ts > 100_000) $  -- 100μs超时
    return $ Left StaleData
  
  return $ Right detections

-- 使用线性类型确保释放
withPerception :: (PerceptionShm ->. IO a) -> IO a
withPerception action = do
  shm <- openSharedMemory "/dev/shm/autonomy_perception"
  result <- action shm
  closeSharedMemory shm  -- 线性类型强制调用
  return result

-- 规划主循环
planningLoop :: PerceptionShm ->. IO ()
planningLoop shm = forever $ do
  -- 硬实时：每10ms一个周期
  tick <- waitForTick 10_000  -- 微秒
  
  -- 获取感知（零拷贝，<1μs）
  perception <- acquirePerception shm
  
  case perception of
    Left err -> do
      -- 故障安全：使用上一帧或保守策略
      executeFallbackStrategy err
    
    Right detections -> do
      -- 确定性规划（Haskell纯函数）
      let trajectory = planTrajectory detections
      
      -- 类型安全：轨迹必须经过验证
      case validateTrajectory trajectory of
        Left unsafe -> executeEmergencyStop unsafe
        Right safe -> sendToController safe
```

### Python 计算平面

```python
# perception_service.py
import mmap
import os
import struct
import numpy as np
import torch
import torch.cuda as cuda
from multiprocessing import shared_memory

class SharedMemoryPerception:
    def __init__(self, shm_name="/autonomy_perception"):
        # 创建共享内存（HugePages，1GB）
        self.shm = shared_memory.SharedMemory(
            name=shm_name,
            create=True,
            size=1024*1024*1024
        )
        
        # 布局：头部 | 点云(200MB) | 图像(300MB) | 输出(100MB)
        self.header = np.ndarray(
            (4,), dtype=np.int64,
            buffer=self.shm.buf[:32]
        )
        self.point_cloud = np.ndarray(
            (500000, 4), dtype=np.float32,
            buffer=self.shm.buf[32:200000032]
        )
        self.image = np.ndarray(
            (1080, 1920, 3), dtype=np.uint8,
            buffer=self.shm.buf[200000032:500000032]
        )
        self.output = np.ndarray(
            (1000, 7), dtype=np.float32,  # x,y,z,w,h,l,class
            buffer=self.shm.buf[500000032:600000032]
        )
        
        # 初始化头部
        self.header[0] = 0   # write_seq
        self.header[1] = 0   # read_seq
        self.header[2] = 0   # timestamp
        self.header[3] = 0   # data_ready
        
        # 加载模型到GPU
        self.model = torch.jit.load("pointpillar_cuda.pt")
        self.model.cuda()
        
    def process_frame(self, lidar_data, camera_data):
        """处理一帧传感器数据"""
        
        # 1. 序列号原子递增
        write_seq = self.header[0] + 1
        
        # 2. 检查Haskell是否已读取上一帧
        if write_seq - self.header[1] > 1:
            # Haskell处理慢，丢帧或告警
            self.log_warning("Haskell lagging, dropping frame")
            return
        
        # 3. 零拷贝：传感器数据直接写入共享内存
        np.copyto(self.point_cloud, lidar_data)
        np.copyto(self.image, camera_data)
        
        # 4. GPU推理（CUDA流，异步）
        with torch.cuda.stream(self.cuda_stream):
            points = torch.from_numpy(self.point_cloud).cuda()
            detections = self.model(points)  # PointPillars
            
            # 结果写回共享内存（GPU Direct，零拷贝）
            self.output[:] = detections.cpu().numpy()
        
        # 5. 原子更新：时间戳 -> 序列号 -> 就绪标志
        self.header[2] = get_monotonic_ns()  # timestamp
        self.header[0] = write_seq            # write_seq
        torch.cuda._sleep(1)  # 内存屏障
        self.header[3] = 1                    # data_ready = True
        
    def run(self):
        """主循环：与传感器频率同步（10Hz/100Hz）"""
        while True:
            lidar = self.lidar_driver.capture()
            camera = self.camera_driver.capture()
            
            self.process_frame(lidar, camera)
            
            # 精确周期控制
            self.sync_to_next_tick(10_000_000)  # 10ms = 100Hz

# 启动
if __name__ == "__main__":
    perception = SharedMemoryPerception()
    perception.run()
```

### 交互流程图

```
传感器数据 (10GB/s)
   │
   ▼
┌─────────────────┐
│ Python          │
│ 1. 接收原始数据  │
│ 2. 零拷贝写入SHM │◄── mmap，无序列化
│ 3. GPU推理      │◄── PointPillars，3ms
│ 4. 结果写回SHM   │
│ 5. 原子信号就绪  │◄── seq号 + data_ready标志
└────────┬────────┘
         │ 共享内存 (零拷贝，零系统调用)
         │ 延迟: <1μs
         ▼
┌─────────────────┐
│ Haskell         │
│ 1. 轮询seq号    │◄── 无锁读，自旋<1μs
│ 2. 验证时间戳   │◄── 拒绝延迟>100μs数据
│ 3. 零拷贝读取   │◄── castPtr，无反序列化
│ 4. 确定性规划   │◄── 纯函数，无GC暂停
│ 5. 输出控制指令  │
└─────────────────┘
         │
         ▼
    车辆执行器
```

---

## 5. 交互模式对比

| 模式 | 延迟 | 吞吐 | 复杂度 | 适用场景 |
|:---|:---|:---|:---|:---|
| **请求-响应** | 5-50ms | 10K QPS | 低 | 风控、推荐、对话 |
| **流式传输** | 首包50ms，逐字<10ms | 无限 | 中 | LLM生成、日志处理 |
| **共享内存** | <10μs | 10GB/s | 高 | 自动驾驶、高频交易 |

---

## 6. 关键设计原则

### 6.1 防御性编程

```haskell
-- Haskell端：永远不信任Python输出
validatePythonOutput :: RawOutput -> Either ValidationError SafeOutput
validatePythonOutput raw = do
  -- 1. 结构验证：符合预期schema？
  struct <- parseJSON raw ?~> ParseError
  
  -- 2. 范围验证：数值在合理区间？
  value <- checkRange (minBound, maxBound) struct.value ?~> OutOfRange
  
  -- 3. 一致性验证：与上下文一致？
  checkConsistency value context ?~> Inconsistent
  
  -- 4. 安全验证：无注入/溢出？
  sanitize value ?~> SecurityViolation
  
  return value
```

### 6.2 故障隔离

| 故障场景 | Haskell行为 | Python行为 |
|:---|:---|:---|
| Python崩溃 | 熔断，路由到其他实例，记录审计 | 自动重启，恢复状态 |
| 网络中断 | 重试+退避，最终降级到本地规则 | 无感知，继续处理本地队列 |
| 超时 | 返回保守决策，标记为"模型不可用" | 中断推理，释放资源 |
| 数据格式错误 | 拒绝执行，告警，人工介入 | 记录详细错误，便于调试 |

### 6.3 可观测性贯通

```
┌─────────────────────────────────────────────────────────────┐
│  跨语言追踪上下文 (OpenTelemetry)                            │
│  TraceID: 0xaf7651916cd43dd8448eb211c80319c3                │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐                 │
│  │ Haskell │───→│  gRPC   │───→│ Python  │                 │
│  │  parse  │    │  encode │    │  infer  │                 │
│  │  2ms    │    │  0.1ms  │    │  15ms   │                 │
│  │         │    │         │    │         │                 │
│  │ SpanID: │    │ SpanID: │    │ SpanID: │                 │
│  │  0x1    │───→│  0x2    │───→│  0x3    │                 │
│  └─────────┘    └─────────┘    └─────────┘                 │
│       ↑______________________________|                      │
│       └──────── 父子关系，完整调用链 ────────┘                 │
└─────────────────────────────────────────────────────────────┘
```

---

## 7. 一句话总结

> **控制平面与计算平面的交互，本质上是"确定性"与"不确定性"的边界管理——Haskell用类型系统守护边界，Python在边界内自由发挥智能，两者通过零拷贝、流式、共享内存等技术实现高效协作。**