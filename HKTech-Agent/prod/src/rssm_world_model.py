#!/usr/bin/env python3
"""
Lightweight RSSM World Model for HKTech Agent
è½»é‡çº§ä¸–ç•Œæ¨¡å‹ - å¯åœ¨CPUä¸Šè®­ç»ƒ

åŸºäº DreamerV2/V3 çš„ç®€åŒ–å®ç°
å‚æ•°é‡: ~150K (å¯åœ¨CPUä¸Šå¿«é€Ÿè®­ç»ƒ)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
import random

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

set_seed()


class RSSM(nn.Module):
    """
    Recurrent State-Space Model
    æ ¸å¿ƒä¸–ç•Œæ¨¡å‹ç»„ä»¶
    
    ç»“æ„:
    - Recurrent Model (h): GRUå¤„ç†æ—¶åº
    - Representation (z): å˜åˆ†ç¼–ç è§‚æµ‹
    - Transition (prior): é¢„æµ‹ä¸‹ä¸€çŠ¶æ€
    - Observation (decoder): é‡å»ºè§‚æµ‹
    - Reward Model: é¢„æµ‹æ”¶ç›Š
    """
    
    def __init__(self, 
                 obs_dim=15,      # è§‚æµ‹ç»´åº¦ (ä»·æ ¼,æŠ€æœ¯æŒ‡æ ‡,æŒä»“ç­‰)
                 action_dim=3,    # åŠ¨ä½œç»´åº¦ (3åªè‚¡ç¥¨çš„ç›®æ ‡ä»“ä½å˜åŒ–)
                 hidden_dim=64,   # éšè—å±‚ç»´åº¦ (å°æ¨¡å‹ç”¨64ï¼Œå¤§å¯128)
                 latent_dim=32,   # æ½œå˜é‡ç»´åº¦
                 latent_classes=32):  # ç¦»æ•£æ½œå˜é‡ç±»åˆ«æ•°
        super().__init__()
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.latent_classes = latent_classes
        self.latent_flat_dim = latent_dim * latent_classes
        
        # 1. Recurrent Model (h_t+1 = f(h_t, z_t, a_t))
        # è¾“å…¥: [hidden + latent_flat + action]
        self.gru = nn.GRUCell(
            input_size=self.latent_flat_dim + action_dim,
            hidden_size=hidden_dim
        )
        
        # 2. Representation Model (q(z_t | h_t, o_t))
        # ä»è§‚æµ‹ç¼–ç æ½œå˜é‡
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim + hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim * latent_classes)
        )
        
        # 3. Transition/Prior Model (p(z_t | h_t))
        # ä»éšè—çŠ¶æ€é¢„æµ‹æ½œå˜é‡ï¼ˆæƒ³è±¡æ—¶ç”¨ï¼‰
        self.prior = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim * latent_classes)
        )
        
        # 4. Observation Decoder (p(o_t | h_t, z_t))
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim + self.latent_flat_dim, 128),
            nn.ReLU(),
            nn.Linear(128, obs_dim)
        )
        
        # 5. Reward Predictor (p(r_t | h_t, z_t))
        self.reward_model = nn.Sequential(
            nn.Linear(hidden_dim + self.latent_flat_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # 6. Continue Predictor (p(cont | h_t, z_t)) - æ˜¯å¦ç»ˆæ­¢
        self.continue_model = nn.Sequential(
            nn.Linear(hidden_dim + self.latent_flat_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def encode(self, obs: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç è§‚æµ‹ä¸ºæ½œå˜é‡ (q(z|h,o))
        è¿”å› logits
        """
        x = torch.cat([obs, h], dim=-1)
        logits = self.encoder(x)
        # reshape: [batch, latent_dim, latent_classes]
        logits = logits.view(-1, self.latent_dim, self.latent_classes)
        return logits
    
    def dynamics(self, h: torch.Tensor, z: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        åŠ¨åŠ›å­¦æ¨¡å‹: h_t+1 = GRU(h_t, [z_t, a_t])
        """
        x = torch.cat([z, action], dim=-1)
        h_next = self.gru(x, h)
        return h_next
    
    def imagine_prior(self, h: torch.Tensor) -> torch.Tensor:
        """
        å…ˆéªŒé¢„æµ‹: p(z|h)ï¼Œç”¨äºæƒ³è±¡æœªæ¥
        """
        logits = self.prior(h)
        logits = logits.view(-1, self.latent_dim, self.latent_classes)
        return logits
    
    def decode(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:
        """
        è§£ç è§‚æµ‹: p(o|h,z)
        """
        x = torch.cat([h, z], dim=-1)
        obs = self.decoder(x)
        return obs
    
    def predict_reward(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹å¥–åŠ±
        """
        x = torch.cat([h, z], dim=-1)
        reward = self.reward_model(x)
        return reward
    
    def predict_continue(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹æ˜¯å¦ç»§ç»­ï¼ˆéç»ˆæ­¢æ¦‚ç‡ï¼‰
        """
        x = torch.cat([h, z], dim=-1)
        cont = self.continue_model(x)
        return cont
    
    def sample_z(self, logits: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä»logitsé‡‡æ ·æ½œå˜é‡ï¼Œä½¿ç”¨Gumbel-Softmaxï¼ˆå¯å¾®åˆ†ï¼‰
        
        è¿”å›:
            z: [batch, latent_flat_dim] é‡‡æ ·ç»“æœ
            z_dist: [batch, latent_dim, latent_classes] åˆ†å¸ƒ
        """
        # ä½¿ç”¨softmaxè·å–åˆ†å¸ƒ
        z_dist = F.softmax(logits, dim=-1)
        
        # Gumbel-Softmaxé‡‡æ ·ï¼ˆè®­ç»ƒæ—¶ï¼‰
        if self.training:
            # é‡å‚æ•°åŒ–æŠ€å·§
            u = torch.rand_like(logits)
            gumbel = -torch.log(-torch.log(u + 1e-8) + 1e-8)
            z_sample = F.softmax((logits + gumbel) / 0.5, dim=-1)  # temperature=0.5
        else:
            # æ¨ç†æ—¶ç›´æ¥ç”¨argmax
            z_sample = z_dist
        
        # å±•å¹³ä¸º [batch, latent_dim * latent_classes]
        z_flat = z_sample.view(-1, self.latent_flat_dim)
        
        return z_flat, z_dist


class ActorCritic(nn.Module):
    """
    ç­–ç•¥-ä»·å€¼ç½‘ç»œ (SACé£æ ¼)
    """
    
    def __init__(self, hidden_dim=64, latent_flat_dim=1024, action_dim=3):
        super().__init__()
        
        input_dim = hidden_dim + latent_flat_dim
        
        # Actor (ç­–ç•¥ç½‘ç»œ) - è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ
        self.actor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim * 2)  # mean, log_std
        )
        
        # Critic (ä»·å€¼ç½‘ç»œ) - åŒQç½‘ç»œ
        self.critic1 = nn.Sequential(
            nn.Linear(input_dim + action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.critic2 = nn.Sequential(
            nn.Linear(input_dim + action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def get_action(self, state: torch.Tensor, deterministic=False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é‡‡æ ·åŠ¨ä½œ
        
        è¿”å›:
            action: [batch, action_dim]
            log_prob: [batch, 1]
        """
        output = self.actor(state)
        mean, log_std = output.chunk(2, dim=-1)
        log_std = torch.clamp(log_std, -20, 2)
        std = torch.exp(log_std)
        
        if deterministic:
            action = torch.tanh(mean)
            log_prob = None
        else:
            # é‡å‚æ•°åŒ–é‡‡æ ·
            noise = torch.randn_like(mean)
            raw_action = mean + std * noise
            action = torch.tanh(raw_action)
            
            # è®¡ç®—log_prob (å«tanhä¿®æ­£)
            log_prob = -0.5 * ((raw_action - mean) / (std + 1e-8)).pow(2) - log_std - 0.5 * np.log(2 * np.pi)
            log_prob = log_prob.sum(dim=-1, keepdim=True)
            log_prob -= (2 * (np.log(2) - raw_action - F.softplus(-2 * raw_action))).sum(dim=-1, keepdim=True)
        
        return action, log_prob
    
    def get_value(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ä¼°è®¡Qå€¼
        """
        x = torch.cat([state, action], dim=-1)
        q1 = self.critic1(x)
        q2 = self.critic2(x)
        return q1, q2


class WorldModelTrainer:
    """
    ä¸–ç•Œæ¨¡å‹è®­ç»ƒå™¨
    """
    
    def __init__(self, data_dir="/opt/hktech-agent/data", device="cpu"):
        self.data_dir = data_dir
        self.device = torch.device(device)
        
        # æ¨¡å‹
        self.rssm = RSSM(obs_dim=15, action_dim=3, hidden_dim=64).to(device)
        self.actor_critic = ActorCritic(hidden_dim=64, latent_flat_dim=1024, action_dim=3).to(device)
        
        # ä¼˜åŒ–å™¨
        self.world_optimizer = torch.optim.Adam(self.rssm.parameters(), lr=1e-3)
        self.actor_optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=3e-4)
        
        # è¶…å‚æ•°
        self.batch_size = 16
        self.seq_len = 10  # åºåˆ—é•¿åº¦ï¼ˆ10å¤©ï¼‰
        self.imagine_horizon = 5  # æƒ³è±¡æ­¥æ•°
        
        self.model_path = f"{data_dir}/rssm_model.pt"
    
    def prepare_data(self, market_data: Dict, portfolio: Dict) -> np.ndarray:
        """
        å°†å¸‚åœºæ•°æ®è½¬æ¢ä¸ºè§‚æµ‹å‘é‡
        
        è§‚æµ‹ç»´åº¦ (15ç»´):
        - 3åªè‚¡ç¥¨: å½“å‰ä»·, MA5, MA20, RSI, æ¶¨è·Œå¹… (15ç»´)
        - å¯é€‰: æŒä»“æ¯”ä¾‹, ç°é‡‘æ¯”ä¾‹
        """
        obs_list = []
        
        for code in ["00700", "09988", "03690"]:
            if code in market_data:
                data = market_data[code]
                obs_list.extend([
                    data.get('price', 0) / 500,  # å½’ä¸€åŒ–
                    data.get('ma5', 0) / 500,
                    data.get('ma20', 0) / 500,
                    data.get('rsi', 50) / 100,
                    data.get('change_pct', 0) / 10
                ])
            else:
                obs_list.extend([0, 0, 0, 0.5, 0])
        
        return np.array(obs_list, dtype=np.float32)
    
    def train_world_model(self, episodes: List[Dict], epochs=50):
        """
        è®­ç»ƒä¸–ç•Œæ¨¡å‹ (ç›‘ç£å­¦ä¹ )
        
        episodes: [{'obs': [], 'action': [], 'reward': []}, ...]
        """
        print(f"ğŸ§  è®­ç»ƒä¸–ç•Œæ¨¡å‹ ({epochs} epochs)...")
        
        losses = []
        for epoch in range(epochs):
            epoch_loss = 0
            
            # éšæœºé‡‡æ ·batch
            batch_episodes = random.sample(episodes, min(self.batch_size, len(episodes)))
            
            for ep in batch_episodes:
                obs_seq = torch.tensor(ep['obs'][:self.seq_len], dtype=torch.float32).to(self.device)
                action_seq = torch.tensor(ep['action'][:self.seq_len], dtype=torch.float32).to(self.device)
                reward_seq = torch.tensor(ep['reward'][:self.seq_len], dtype=torch.float32).to(self.device)
                
                # åˆå§‹åŒ–éšè—çŠ¶æ€
                h = torch.zeros(1, self.rssm.hidden_dim).to(self.device)
                
                total_loss = 0
                kl_losses = []
                obs_losses = []
                reward_losses = []
                
                for t in range(len(obs_seq) - 1):
                    obs_t = obs_seq[t:t+1]
                    obs_next = obs_seq[t+1:t+1]
                    action_t = action_seq[t:t+1]
                    reward_t = reward_seq[t:t+1]
                    
                    # ç¼–ç å½“å‰è§‚æµ‹
                    z_logits = self.rssm.encode(obs_t, h)
                    z, z_dist = self.rssm.sample_z(z_logits)
                    
                    # åŠ¨åŠ›å­¦é¢„æµ‹ä¸‹ä¸€çŠ¶æ€
                    h_next = self.rssm.dynamics(h, z, action_t)
                    
                    # é‡å»ºè§‚æµ‹
                    obs_pred = self.rssm.decode(h, z)
                    obs_loss = F.mse_loss(obs_pred, obs_next)
                    
                    # é¢„æµ‹å¥–åŠ±
                    reward_pred = self.rssm.predict_reward(h, z)
                    reward_loss = F.mse_loss(reward_pred, reward_t)
                    
                    # KLæ•£åº¦ (ä¸å…ˆéªŒå¯¹æ¯”)
                    prior_logits = self.rssm.imagine_prior(h)
                    prior_dist = F.softmax(prior_logits, dim=-1)
                    kl_loss = F.kl_div(z_dist.log(), prior_dist, reduction='batchmean')
                    
                    # æ€»æŸå¤±
                    loss = obs_loss + 0.1 * reward_loss + 0.001 * kl_loss
                    total_loss += loss
                    
                    kl_losses.append(kl_loss.item())
                    obs_losses.append(obs_loss.item())
                    reward_losses.append(reward_loss.item())
                    
                    h = h_next
                
                # åå‘ä¼ æ’­
                self.world_optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.rssm.parameters(), 100)
                self.world_optimizer.step()
                
                epoch_loss += total_loss.item()
            
            losses.append(epoch_loss / len(batch_episodes))
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}, "
                      f"KL: {np.mean(kl_losses):.4f}, "
                      f"Obs: {np.mean(obs_losses):.4f}, "
                      f"Reward: {np.mean(reward_losses):.4f}")
        
        print(f"âœ… ä¸–ç•Œæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆLoss: {losses[-1]:.4f}")
        return losses
    
    def imagine_future(self, initial_obs: np.ndarray, initial_action: np.ndarray, horizon=5) -> Dict:
        """
        æƒ³è±¡æœªæ¥ (æ ¸å¿ƒåŠŸèƒ½)
        
        è¿”å›é¢„æµ‹çš„æœªæ¥è½¨è¿¹
        """
        self.rssm.eval()
        
        with torch.no_grad():
            obs = torch.tensor(initial_obs, dtype=torch.float32).unsqueeze(0).to(self.device)
            action = torch.tensor(initial_action, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # åˆå§‹åŒ–
            h = torch.zeros(1, self.rssm.hidden_dim).to(self.device)
            z_logits = self.rssm.encode(obs, h)
            z, _ = self.rssm.sample_z(z_logits)
            
            # æƒ³è±¡æœªæ¥
            imagined_trajectory = []
            
            for t in range(horizon):
                # åŠ¨åŠ›å­¦é¢„æµ‹
                h = self.rssm.dynamics(h, z, action)
                
                # ç”¨å…ˆéªŒé¢„æµ‹ä¸‹ä¸€æ½œå˜é‡
                prior_logits = self.rssm.imagine_prior(h)
                z, _ = self.rssm.sample_z(prior_logits)
                
                # è§£ç è§‚æµ‹
                obs_pred = self.rssm.decode(h, z)
                
                # é¢„æµ‹å¥–åŠ±
                reward_pred = self.rssm.predict_reward(h, z)
                
                # ç”¨actoré¢„æµ‹ä¸‹ä¸€åŠ¨ä½œ
                state = torch.cat([h, z], dim=-1)
                action, _ = self.actor_critic.get_action(state, deterministic=True)
                
                imagined_trajectory.append({
                    'step': t,
                    'predicted_obs': obs_pred.cpu().numpy()[0],
                    'predicted_reward': reward_pred.cpu().numpy()[0][0],
                    'action': action.cpu().numpy()[0]
                })
            
            return {
                'horizon': horizon,
                'trajectory': imagined_trajectory,
                'cumulative_reward': sum([t['predicted_reward'] for t in imagined_trajectory])
            }
    
    def save(self):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'rssm': self.rssm.state_dict(),
            'actor_critic': self.actor_critic.state_dict(),
            'world_optimizer': self.world_optimizer.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict()
        }, self.model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
    
    def load(self):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(self.model_path):
            checkpoint = torch.load(self.model_path, map_location=self.device)
            self.rssm.load_state_dict(checkpoint['rssm'])
            self.actor_critic.load_state_dict(checkpoint['actor_critic'])
            self.world_optimizer.load_state_dict(checkpoint['world_optimizer'])
            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
            print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {self.model_path}")
            return True
        return False


def generate_dummy_data(num_episodes=20, seq_len=15):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ® (ç”¨äºæµ‹è¯•)
    å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®å†å²æ•°æ®
    """
    episodes = []
    
    for _ in range(num_episodes):
        obs_seq = []
        action_seq = []
        reward_seq = []
        
        # éšæœºæ¸¸èµ°ä»·æ ¼
        price = 400
        for _ in range(seq_len):
            # æ¨¡æ‹Ÿè§‚æµ‹ [è…¾è®¯ä»·æ ¼/500, MA5/500, MA20/500, RSI/100, æ¶¨è·Œå¹…/10, ...]
            obs = [
                price/500, (price*0.98)/500, (price*0.95)/500, 0.5, 0.01,
                (price*0.9)/500, (price*0.88)/500, (price*0.85)/500, 0.45, 0.02,
                (price*1.1)/500, (price*1.08)/500, (price*1.05)/500, 0.55, -0.01
            ]
            obs_seq.append(obs)
            
            # éšæœºåŠ¨ä½œ (3åªè‚¡ç¥¨çš„ç›®æ ‡ä»“ä½)
            action = np.random.randn(3) * 0.1
            action_seq.append(action)
            
            # æ¨¡æ‹Ÿæ”¶ç›Š
            reward = np.random.randn() * 0.01
            reward_seq.append([reward])
            
            # ä»·æ ¼éšæœºæ¸¸èµ°
            price *= (1 + np.random.randn() * 0.02)
        
        episodes.append({
            'obs': obs_seq,
            'action': action_seq,
            'reward': reward_seq
        })
    
    return episodes


def test_world_model():
    """æµ‹è¯•ä¸–ç•Œæ¨¡å‹"""
    print("="*50)
    print("ğŸ§ª æµ‹è¯• RSSM ä¸–ç•Œæ¨¡å‹")
    print("="*50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = WorldModelTrainer(device="cpu")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")
    episodes = generate_dummy_data(num_episodes=20, seq_len=15)
    print(f"âœ… ç”Ÿæˆ {len(episodes)} æ¡è®­ç»ƒåºåˆ—")
    
    # è®­ç»ƒä¸–ç•Œæ¨¡å‹
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train_world_model(episodes, epochs=50)
    
    # ä¿å­˜
    trainer.save()
    
    # æµ‹è¯•æƒ³è±¡åŠŸèƒ½
    print("\nğŸ”® æµ‹è¯•æƒ³è±¡åŠŸèƒ½...")
    initial_obs = episodes[0]['obs'][0]
    initial_action = episodes[0]['action'][0]
    
    prediction = trainer.imagine_future(initial_obs, initial_action, horizon=5)
    
    print(f"\né¢„æµ‹æœªæ¥ {prediction['horizon']} æ­¥:")
    for step in prediction['trajectory']:
        print(f"  Step {step['step']}: é¢„æµ‹æ”¶ç›Š={step['predicted_reward']:.4f}, "
              f"åŠ¨ä½œ={[f'{a:.2f}' for a in step['action']]}")
    
    print(f"\nç´¯è®¡é¢„æµ‹æ”¶ç›Š: {prediction['cumulative_reward']:.4f}")
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
    
    return trainer


if __name__ == "__main__":
    test_world_model()
