#!/usr/bin/env python3
"""
GRU World Model è®­ç»ƒè„šæœ¬
æ•°æ®: 2018-è‡³ä»Š, 6åªæ’ç§‘è‚¡ç¥¨
æ¨¡å‹: GRU(input=8, hidden=64, layers=2)
"""
import os
import sys
import numpy as np
import pandas as pd

_HERE = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(_HERE, "../../shared"))
sys.path.insert(0, _HERE)

try:
    from constants import ALL_STOCKS, STOCKS as STOCK_INFO, get_data_dir
    TRAIN_STOCKS = {code: STOCK_INFO[code] for code in ALL_STOCKS if code in STOCK_INFO}
except Exception:
    TRAIN_STOCKS = {
        "00700": {"name": "è…¾è®¯æ§è‚¡"}, "09988": {"name": "é˜¿é‡Œå·´å·´"},
        "03690": {"name": "ç¾å›¢-W"}, "01810": {"name": "å°ç±³é›†å›¢"},
        "09618": {"name": "äº¬ä¸œé›†å›¢"}, "09999": {"name": "ç½‘æ˜“"},
    }
    def get_data_dir():
        return os.path.join(_HERE, "../../data")


def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    delta = prices.diff()
    gain = delta.clip(lower=0).rolling(period).mean()
    loss = (-delta.clip(upper=0)).rolling(period).mean()
    rs = gain / (loss + 1e-8)
    return 100 - (100 / (1 + rs))


def _generate_mock_historical() -> dict:
    """ç”Ÿæˆ mock å†å²æ•°æ®ï¼ˆç”¨äºæ— ç½‘ç»œç¯å¢ƒï¼‰"""
    result = {}
    dates = pd.date_range("2018-01-02", periods=1500, freq="B")
    for code in TRAIN_STOCKS:
        rng = np.random.RandomState(hash(code) % 2**31)
        prices = 100 * np.cumprod(1 + rng.normal(0, 0.015, 1500))
        df = pd.DataFrame({
            "Open":   prices * rng.uniform(0.99, 1.0, 1500),
            "High":   prices * rng.uniform(1.0, 1.02, 1500),
            "Low":    prices * rng.uniform(0.98, 1.0, 1500),
            "Close":  prices,
            "Volume": rng.uniform(1e7, 5e7, 1500),
        }, index=dates)
        result[code] = df
    return result


def fetch_historical_data(start_date="2018-01-01") -> dict:
    """æ‹‰å– start_date è‡³ä»Šçš„å†å²æ•°æ®ï¼Œå¤±è´¥æ—¶ç”¨ mock"""
    try:
        import yfinance as yf
    except ImportError:
        print("âš ï¸ yfinance ä¸å¯ç”¨ï¼Œä½¿ç”¨ mock æ•°æ®")
        return _generate_mock_historical()

    from datetime import datetime
    end_date = datetime.now().strftime("%Y-%m-%d")
    result = {}
    for code, info in TRAIN_STOCKS.items():
        yf_symbol = f"{int(code):04d}.HK"
        try:
            df = yf.download(yf_symbol, start=start_date, end=end_date,
                             progress=False, auto_adjust=True)
            if len(df) >= 100:
                result[code] = df
                print(f"âœ… {code} {info.get('name', '')}: {len(df)} æ¡è®°å½•")
            else:
                print(f"âš ï¸ {code}: æ•°æ®ä¸è¶³ ({len(df)}æ¡), è·³è¿‡")
        except Exception as e:
            print(f"âŒ {code}: ä¸‹è½½å¤±è´¥ {e}")

    if not result:
        print("âš ï¸ æ‰€æœ‰è‚¡ç¥¨ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨ mock æ•°æ®")
        return _generate_mock_historical()
    return result


def build_feature_matrix(df: pd.DataFrame) -> pd.DataFrame:
    """æ„å»º 8 ç»´ç‰¹å¾: open_r, high_r, low_r, close_r, vol_r, ma5_r, rsi_norm, vol_ratio"""
    feat = pd.DataFrame(index=df.index)
    close = df["Close"]
    feat["open_r"]    = (df["Open"] / close.shift(1) - 1).fillna(0)
    feat["high_r"]    = (df["High"] / close - 1).fillna(0)
    feat["low_r"]     = (df["Low"] / close - 1).fillna(0)
    feat["close_r"]   = close.pct_change().fillna(0)
    feat["vol_r"]     = df["Volume"].pct_change().fillna(0)
    ma5 = close.rolling(5).mean()
    feat["ma5_r"]     = (ma5 / close - 1).fillna(0)
    rsi = calculate_rsi(close)
    feat["rsi_norm"]  = ((rsi - 50) / 50).fillna(0)
    avg_vol = df["Volume"].rolling(20).mean()
    feat["vol_ratio"] = (df["Volume"] / avg_vol - 1).fillna(0)
    return feat.clip(-3, 3)


def create_training_dataset(historical_data: dict, seq_len: int = 20) -> tuple:
    """æ„å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†: X (N, seq_len, 8), y (N,) â€” 5æ—¥æ”¶ç›Šç‡"""
    X_list, y_list = [], []
    for code, df in historical_data.items():
        # handle both single-level and multi-level columns from yfinance
        if isinstance(df.columns, pd.MultiIndex):
            df = df.droplevel(1, axis=1)
        feat = build_feature_matrix(df).values.astype(np.float32)
        closes = df["Close"].values.astype(np.float32)
        for i in range(seq_len, len(feat) - 5):
            x = feat[i - seq_len:i]
            y = float(closes[i + 5] / closes[i] - 1)
            if not (np.isnan(x).any() or np.isnan(y) or np.isinf(y)):
                X_list.append(x)
                y_list.append(y)
    if not X_list:
        return np.zeros((0, seq_len, 8), dtype=np.float32), np.zeros(0, dtype=np.float32)
    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.float32)


def train_gru_model(X: np.ndarray, y: np.ndarray, data_dir: str,
                    epochs: int = 100, patience: int = 10) -> str:
    """è®­ç»ƒ GRU ä¸–ç•Œæ¨¡å‹ï¼Œä¿å­˜åˆ° data_dir/models/"""
    try:
        import torch
        import torch.nn as nn
        from torch.utils.data import TensorDataset, DataLoader
        import pickle
    except ImportError:
        print("âŒ torch ä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
        return ""

    from rssm_world_model import GRUWorldModel

    models_dir = os.path.join(data_dir, "models")
    os.makedirs(models_dir, exist_ok=True)

    split = int(len(X) * 0.8)
    X_train, X_val = X[:split], X[split:]
    y_train, y_val = y[:split], y[split:]

    y_mean = float(y_train.mean())
    y_std  = float(y_train.std()) + 1e-8
    y_train_n = (y_train - y_mean) / y_std
    y_val_n   = (y_val   - y_mean) / y_std

    with open(os.path.join(models_dir, "scaler.pkl"), "wb") as f:
        pickle.dump({"y_mean": y_mean, "y_std": y_std}, f)

    train_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_n)),
        batch_size=64, shuffle=True)
    val_loader = DataLoader(
        TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val_n)),
        batch_size=256)

    model = GRUWorldModel(input_size=8, hidden_size=64, num_layers=2)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()

    best_val_loss = float("inf")
    no_improve = 0
    model_path = os.path.join(models_dir, "rssm_model.pt")

    for epoch in range(epochs):
        model.train()
        for xb, yb in train_loader:
            optimizer.zero_grad()
            loss = criterion(model(xb).squeeze(), yb)
            loss.backward()
            optimizer.step()

        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                val_losses.append(criterion(model(xb).squeeze(), yb).item())
        val_loss = sum(val_losses) / len(val_losses)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), model_path)
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"æ—©åœäº epoch {epoch+1}, æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
                break

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}: val_loss={val_loss:.6f}")

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    return model_path


def _upload_to_oss_after_training(data_dir: str, model_path: str) -> None:
    """è®­ç»ƒå®Œæˆåå°†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®ä¸Šä¼ åˆ°é˜¿é‡Œäº‘OSSã€‚oss2æœªå®‰è£…æ—¶é™é»˜è·³è¿‡ã€‚"""
    import sys as _sys

    # AccessKey.csv åœ¨é¡¹ç›®æ ¹ç›®å½• (lamda-ai/)ï¼Œæœ¬æ–‡ä»¶åœ¨ prod/src/ ä¸‹ï¼Œå‘ä¸Šä¸‰çº§
    _script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.normpath(os.path.join(_script_dir, "../../../AccessKey.csv"))

    # å¯¼å…¥ OSSManagerï¼ˆåœ¨ active_src/ï¼‰
    _active_src = os.path.normpath(os.path.join(_script_dir, "../../active_src"))
    if _active_src not in _sys.path:
        _sys.path.insert(0, _active_src)

    try:
        from oss_manager import OSSManager
    except ImportError:
        print("âš ï¸  oss_manager æœªæ‰¾åˆ°ï¼Œè·³è¿‡OSSä¸Šä¼ ")
        return

    try:
        oss = OSSManager(csv_path=csv_path)
    except ImportError:
        print("âš ï¸  oss2 æœªå®‰è£…ï¼Œè·³è¿‡OSSä¸Šä¼ ã€‚è¿è¡Œ: pip install oss2")
        return
    except ValueError as e:
        print(f"âš ï¸  OSSé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡ä¸Šä¼ : {e}")
        return
    except Exception as e:
        print(f"âš ï¸  OSSåˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡ä¸Šä¼ : {e}")
        return

    # ä¸Šä¼ æ¨¡å‹æ–‡ä»¶
    models_dir = os.path.join(data_dir, "models")
    for fname in ("rssm_model.pt", "scaler.pkl"):
        fpath = os.path.join(models_dir, fname)
        if os.path.exists(fpath):
            try:
                oss.upload_model(fpath, fname)
            except Exception as e:
                print(f"âš ï¸  ä¸Šä¼  {fname} å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {fpath}")

    # ä¸Šä¼ è®­ç»ƒæ•°æ®
    episodes_path = os.path.join(data_dir, "training_episodes.json")
    if os.path.exists(episodes_path):
        try:
            oss.upload_training_data(episodes_path, "training_episodes.json")
        except Exception as e:
            print(f"âš ï¸  ä¸Šä¼  training_episodes.json å¤±è´¥: {e}")

    print("âœ… OSSä¸Šä¼ æµç¨‹å®Œæˆ")


def main():
    data_dir = str(get_data_dir())
    print("ğŸ“¥ æ‹‰å–å†å²æ•°æ®ï¼ˆ2018-è‡³ä»Šï¼‰...")
    historical_data = fetch_historical_data(start_date="2018-01-01")

    if not historical_data:
        print("âŒ æ— æ³•è·å–å†å²æ•°æ®ï¼Œç»ˆæ­¢è®­ç»ƒ")
        return

    print("ğŸ”§ æ„å»ºè®­ç»ƒæ•°æ®é›† (seq_len=20)...")
    X, y = create_training_dataset(historical_data, seq_len=20)
    if len(X) == 0:
        print("âŒ æ ·æœ¬æ•°ä¸º 0ï¼Œæ£€æŸ¥æ•°æ®è´¨é‡")
        return
    print(f"   æ ·æœ¬æ•°: {len(X)}, X shape: {X.shape}, y å‡å€¼: {y.mean():.4f}")

    print("ğŸš€ å¼€å§‹è®­ç»ƒ GRU ä¸–ç•Œæ¨¡å‹...")
    model_path = train_gru_model(X, y, data_dir)

    # è®­ç»ƒæˆåŠŸåè‡ªåŠ¨ä¸Šä¼ åˆ°OSS
    if model_path:
        _upload_to_oss_after_training(data_dir, model_path)

    if model_path:
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹: {model_path}")
    else:
        print("âš ï¸ è®­ç»ƒè·³è¿‡ï¼ˆtorch ä¸å¯ç”¨ï¼‰")


if __name__ == "__main__":
    main()
